\documentclass{article}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}

\title{The Invariance of Dithering Efficiency Under Non-Conformal Projections: A Case Study on Octahedral Normal Maps}
\author{Rémi Millerat--Gallot}
\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
    Hemi-Octahedral encoding, standard for normal map compression, significantly distorts surface density, yet uniform dithering remains the industry default. This paper provides a mathematical proof for the invariance of dithering efficiency under such non-conformal projections. By contrasting standard methods against a Jacobian-weighted theoretical foil, we demonstrate that explicit density correction is mathematically redundant. Our analysis reveals that the projection transforms both dithering noise and quantization error identically, preserving the local Signal-to-Noise Ratio. We conclude that standard uniform dithering is the theoretically optimal strategy for masking quantization artifacts.
\end{abstract}

\section{Introduction}

    Real-time rendering heavily relies on optimizing memory bandwidth, making vector quantization essential. Octahedral and Hemi-Octahedral encodings have replaced older methods like spherical coordinates to become the industry standard for G-Buffers, widely adopted by engines such as Unreal Engine 5 and Unity. These methods dominate because they efficiently map 3D unit vectors to a compact 2D domain without the generic ``pole'' artifacts of polar systems. By packing normals into cost-effective formats like RG16 or RG8, they offer the optimal trade-off between storage size and computational cost.

    However, the efficiency of Hemi-Octahedral encoding comes at the cost of geometric uniformity. The mapping from the 2D square domain $\Omega$ to the unit sphere $S^2$ is neither conformal nor area-preserving. Consequently, the pixel density—the surface area on the sphere represented by a single texel—varies significantly across the projection. In regions such as the mid-quadrants, the mapping stretches the domain, causing a fixed quantization step in texture space to manifest as a magnified geometric error on the sphere surface. This results in a spatially inconsistent error distribution, where quantization artifacts (banding) are drastically more visible in specific regions of the view frustum than others.
    
    This non-uniformity invites a compelling question regarding signal processing: why does uniform noise in texture space yield high-quality masking on the sphere, despite the severe geometric distortion? Intuitively, one might assume that a uniform noise floor in texture space would result in non-uniform, ineffective masking on the sphere surface. This intuition would suggest a \textit{Jacobian-weighted} approach: modulating the noise amplitude proportionally to the local distortion to normalize the noise energy.

    This paper provides the formal mathematical proof for why such complexity is unnecessary. We utilize Anisotropic Jacobian-Weighted Dithering (AJWD) as a theoretical baseline to investigate the behavior of noise under non-conformal mappings. Our analysis demonstrates that Uniform PDF Dithering remains the optimal strategy because the projection operator transforms both the dithering noise and the quantization error identically. Consequently, the local Signal-to-Noise Ratio (SNR) remains invariant. We identify the ``Anisotropy Fallacy''—the mistaken belief that isotropic noise is desirable on the sphere surface—and show that standard uniform dithering naturally satisfies the requirement for covariant masking, rendering explicit corrections computationally wasteful.

\section{Related Work}

\subsection{Unit Vector Encodings}
    Efficiently mapping the continuous unit sphere $S^2$ to a discrete 2D domain $\Omega$ is a foundational problem in real-time rendering. Historical approaches relied on spherical coordinates (azimuth and inclination), but this parameterization suffers from the ``singularity problem'': precision density tends toward infinity at the poles while becoming sparse at the equator. As confirmed in the comprehensive survey by Cigolle et al., this non-uniformity creates view-dependent artifacts and wastes significant precision.
    
    To address these inherent geometric inefficiencies, Meyer et al. introduced Octahedral Normal Vectors (ONV) in 2010. By projecting the sphere onto an inscribed octahedron using the $L_1$ norm and unwrapping it to a square, they eliminated the need for expensive transcendental functions while ensuring a nearly uniform distribution of precision. By 2026, Octahedral encoding (and its Hemi-Octahedral variant) has superseded older methods to become the industry gold standard. However, while macroscopically efficient, the mapping is not strictly isometric; it introduces local variations in pixel density (Jacobian determinant) that form the basis of our quantization analysis.

\subsection{Quantization \& Dithering}
    Quantization artifacts, manifesting as Mach bands, are an inherent consequence of truncating high-precision signals to compact formats like 8-bit or 10-bit G-Buffers. Dithering mitigates this by adding noise prior to quantization, effectively decorrelating the discretization error from the signal and converting low-frequency structured artifacts into high-frequency noise.

    The perceptual quality of dithering is governed by two primary properties: the Power Spectral Density (PSD) and the Probability Density Function (PDF). Spatially, modern real-time rendering favors distributions with high-frequency characteristics (such as Blue Noise or Interleaved Gradient Noise) to shift error into frequency bands where the Human Visual System is least sensitive. Regarding amplitude, simple uniform distributions are insufficient; a Triangular Probability Distribution (TPDF) is theoretically required to ensure a constant noise floor and eliminate signal-dependent noise modulation.

\subsection{Perceptual Metrics in Rendering}
    The objective evaluation of image fidelity requires metrics that align with the Human Visual System (HVS), rather than simple pixel-difference calculations. Traditional metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) operate directly on image intensity and correlate poorly with subjective fidelity ratings.

    To address this, metrics like the Structural Similarity (SSIM) index were introduced to capture the loss of structure in an image, based on the hypothesis that the HVS is highly adapted to extract structural information. However, SSIM and its multi-scale variant (MS-SSIM) treat all positions in an image with equal importance during pooling, which fails to account for the varying contributions of different local regions to perception.
    
    For the evaluation of quantization artifacts in this study, we utilize the Feature Similarity Index (FSIM) proposed by Zhang et al.. FSIM is grounded in the physiological evidence that the HVS understands images mainly according to low-level features. The metric employs two primary features to characterize local quality:
    
    \begin{itemize} \item \textbf{Phase Congruency (PC):} A dimensionless measure of the significance of a local structure. PC is chosen because visually discernable features coincide with points where Fourier waves at different frequencies have congruent phases. This allows the model to detect stable features like edges and zero crossings. \item \textbf{Gradient Magnitude (GM):} Since PC is contrast invariant, FSIM employs GM as a secondary feature to encode contrast information, which affects the HVS perception of image quality. \end{itemize}
    
    While originally validated on general image databases, FSIM has demonstrated high consistency with subjective evaluations for specific distortion types relevant to this research. In the TID2008 benchmark, FSIM achieved a Spearman correlation (SROCC) of 0.8564 for "quantization noise" and 0.8566 for "additive Gaussian noise". This suggests the metric is theoretically robust for arbitrating between the structural distortion of quantization banding and the noise floor introduced by dithering strategies. Unlike SSIM, which weights all pixels equally, FSIM uses the PC map to weight the final quality score, positing that areas with high phase congruency (significant structure) are more important to the HVS when evaluating similarity.

\section{Theoretical Analysis}
    \subsection{Geometric Properties of the Projection}
    
    We define the Hemi-Octahedral projection as a mapping $\Phi: \Omega \rightarrow S^2$, where the parametric domain is the square $\Omega = [-1, 1]^2$. The construction flattens the sphere onto an inscribed octahedron (the $L_1$ sphere), and applies a 45-degree rotation to map the resulting diamond shape to the axis-aligned square, before normalizing the result. For a domain coordinate $\mathbf{u} = [u, v]^T$, the intermediate $L_1$ vector is given by $\mathbf{p} = [u, v, 1 - |u| - |v|]^T$ (with appropriate folding for the negative hemisphere), and the final mapping is the normalization $\Phi(\mathbf{u}) = \mathbf{p} / \|\mathbf{p}\|_2$.
    
    To analyze the interaction between this mapping and quantization noise, we must quantify the local geometric distortion. This is fully characterized by the Jacobian matrix of the projection, $\mathbf{J}_{\Phi}(\mathbf{u}) = [\frac{\partial \Phi}{\partial u}, \frac{\partial \Phi}{\partial v}]$. Two specific properties of $\mathbf{J}_{\Phi}$ are critical to our analysis:
    
    \begin{enumerate}
        \item \textbf{Area Scaling (Density):} The determinant $J(\mathbf{u}) = \sqrt{\det(\mathbf{J}_{\Phi}^T \mathbf{J}_{\Phi})}$ represents the local expansion factor. As shown in Figure \ref{fig:hemi_distortion}, the mapping is strictly non-uniform. The density fluctuates from $J \approx 0.6$ at the pole ($\mathbf{u}=0$) to $J \approx 2.55$ in the "mid-quadrant" regions, indicating that precision density varies by a factor of $4\times$ across the view frustum.
        \begin{figure}[h]
            \centering
            \includegraphics[height=7cm]{hemi_oct_distortion_plot.png}
            \caption{Heatmap of the Jacobian Determinant $J(\mathbf{u})$ for Hemi-Octahedral projection.}
            \label{fig:hemi_distortion}
        \end{figure}

        \item \textbf{Anisotropy (Shape):} Crucially, the mapping is not conformal. Outside of the center, and specifically around the diagonals, the condition number of $\mathbf{J}_{\Phi}$ increases, implying that the basis vectors are orthogonal neither in direction nor in magnitude. This anisotropic distortion stretches the local geometry significantly along one principal axis.
        \begin{figure}[h]
            \centering
            \includegraphics[height=7cm]{hemi_oct_anisotropy_plot.png}
            \caption{Visualization of Anisotropy in the Hemi-Octahedral projection.}
            \label{fig:hemi_anisotropy}
        \end{figure}
    \end{enumerate}
    
    Consequently, a square quantization cell (or a uniform noise kernel) in the texture domain $\Omega$ does not map to a square patch on the sphere $S^2$. Instead, it is transformed into an elongated, parallelogram-shaped streak aligned with the major eigenvector of $\mathbf{J}_{\Phi}$. This structural deformation is the primary factor governing artifact visibility.
    
\subsection{Signal-to-Noise Invariance and the Anisotropy Fallacy}
    
    Having characterized the distortion, we now address the central hypothesis of this study: whether the dithering noise amplitude should be spatially modulated (weighted by $J(\mathbf{u})^{-1}$) to compensate for the variable density of the projection.
    
    We model the quantization process on the texture grid as the addition of a discretization error vector $\mathbf{e}_{\Omega}$ and a dithering noise vector $\mathbf{\eta}_{\Omega}$. Since the quantization step $\Delta$ is infinitesimal relative to the curvature of the manifold, the projection $\Phi$ is well-approximated by its first-order Taylor expansion. The resulting geometric error vectors on the sphere, $\mathbf{e}_{S^2}$ and $\mathbf{\eta}_{S^2}$, are thus linear transformations of their domain counterparts:
    
    \begin{equation}
        \mathbf{e}_{S^2} \approx \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{e}_{\Omega} \quad \text{and} \quad \mathbf{\eta}_{S^2} \approx \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{\eta}_{\Omega}
    \end{equation}
    
    This linearity reveals two fundamental reasons why explicit density correction is theoretically unsound.

    \subsubsection{Magnitude Invariance}
    
    The efficacy of dithering is determined by the local Signal-to-Noise Ratio (SNR), defined here as the ratio between the magnitude of the projected noise and the magnitude of the projected quantization step. Substituting the linear approximations, we observe that the Jacobian operator scales both terms:
    
    \begin{equation}
        \text{SNR}(\mathbf{u}) = \frac{\| \mathbf{\eta}_{S^2} \|_2}{\| \mathbf{e}_{S^2} \|_2} \approx \frac{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{\eta}_{\Omega} \|_2}{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{e}_{\Omega} \|_2}
    \end{equation}
    
    Since $\mathbf{e}_{\Omega}$ and $\eta_{\Omega}$ are defined in the same parametric space, the operator $\mathbf{J}_{\Phi}$ deforms their respective probability density functions identically.

    To quantify the stability of this relationship, we define the \textit{Efficiency Ratio} $R(\mathbf{u})$ as the normalized quotient of the noise expansion factor to the error expansion factor along the principal geometric axes:
    \begin{equation}
        R(\mathbf{u}) = \frac{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{\eta}_{\Omega} \|_2 / \| \mathbf{\eta}_{\Omega} \|_2}{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{e}_{\Omega} \|_2 / \| \mathbf{e}_{\Omega} \|_2}
    \end{equation}
    
    As validated in Figure \ref{fig:efficiency_ratio}, the efficiency ratio remains stochastically invariant across the domain. The expansion or compression of the surface ($J(\mathbf{u})$) automatically modulates the noise level in exact proportion to the error level. No external weighting is required to maintain the masking threshold.

    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{efficiency_ratio_plot.png}
        \caption{Scatter plot of the Efficiency Ratio $R(\mathbf{u})$ across the domain. The ratio is invariant at $\approx 1.0$, indicating that projected noise scales exactly with projected error.}
        \label{fig:efficiency_ratio}
    \end{figure}

    \subsubsection{The Anisotropy Fallacy}
    
    The failure of density-corrected dithering is most acute when considering the \textit{shape} of the artifacts. The \textit{Jacobian-Weighted} strategy operates on the intuition that noise on the sphere should be isotropic (circular). However, as established in Section 3.1, the quantization error $\mathbf{e}_{S^2}$ is highly anisotropic; it manifests as elongated "streaks" aligned with the major eigenvector of $\mathbf{J}_{\Phi}$.
    
    Optimal masking requires the noise footprint to geometrically match the error footprint. We contrast the two approaches:
    
    \begin{itemize}
        \item \textbf{Uniform Dithering (Optimal):} The uniform noise $\mathbf{\eta}_{\Omega}$ is naturally stretched by $\mathbf{J}_{\Phi}$ into the same anisotropic shape as the error. The noise "streak" perfectly overlays the quantization "streak," ensuring consistent coverage.
        \item \textbf{Isotropic Correction (Sub-optimal):} By attempting to force the projected noise $\mathbf{\eta}_{S^2}$ to be circular, this strategy introduces a geometric mismatch. Along the axis of stretching, the circular noise is insufficiently long to cover the elongated banding artifact (under-dithering), while along the axis of compression, the noise is excessively wide (over-dithering).
    \end{itemize}
    
    We conclude that uniform dithering in the parameter space is the Pareto-optimal strategy. The geometric distortion acts not as a hindrance, but as an automatic spatial modulator that preserves the necessary covariance between signal and noise.

\section{Experimental Methodology}

\subsection{Implementation}

    To empirically validate the theoretical invariance proposed in Section 3, we developed a custom high-precision rendering framework using the WebGPU API and Three.js. The testing environment is designed to isolate quantization artifacts from other sources of error (such as geometric aliasing or texture filtering) and to provide a ``stress test'' scenario where normal precision is the sole determinant of visual quality.
    
    \subsubsection{Rendering Pipeline \& Stress Test Configuration}
    The test scene consists of a highly tessellated sphere ($256 \times 256$ segments) to ensure that vertex interpolation artifacts remain negligible compared to pixel-stage quantization errors. We employ a Blinn-Phong specular model with a high exponent to simulate a low-roughness material (linear roughness $\alpha \approx 0.15$, mapping to a Blinn-Phong exponent of $s \approx 4000$). This configuration serves as the worst-case scenario for normal mapping: narrow specular highlights act as high-frequency amplifiers for low-frequency quantization steps, rendering ``banding'' artifacts immediately visible to the human eye.
    
    The quantization pipeline is implemented in a custom WGSL fragment shader. Instead of relying on hardware texture formats (which offer fixed precision), we simulate arbitrary bit-depths programmatically. The high-precision view-space normal $\mathbf{n}_{view}$ is projected into the Hemi-Octahedral domain $\Omega$, resulting in continuous UV coordinates. We then apply dithering noise $\mathbf{\eta}$ and round the signal to simulating $k$-bit fixed-point storage:
    
    \begin{equation}
        \mathbf{u}_{quant} = \frac{round(\frac{\Phi(\mathbf{n}_{view})+1}{2} \cdot (2^k - 1)  + \mathbf{\eta})}{2^k - 1}
    \end{equation}
    
    The quantized coordinate $\mathbf{u}_{quant}$ is then decoded back to a unit vector $\mathbf{n}_{decoded}$ for lighting calculations. This ``in-silico'' quantization allows us to instantly toggle between 8-bit, 10-bit, and 16-bit precision. Furthermore, because the simulation operates on floating-point bit-depths, we are able to sweep precision continuously (e.g., assessing transition artifacts at 9.5 bits) rather than being constrained to the integer steps of hardware formats.
    
    \subsubsection{The Theoretical Foil: Anisotropic Jacobian-Weighted Dithering (AJWD)}
    To rigorously validate the robustness of standard dithering, we implemented a counter-factual algorithm: Anisotropic Jacobian-Weighted Dithering (AJWD). While not a standard industrial technique, AJWD serves as the rigorous embodiment of the "isotropic intuition"—the hypothesis that noise should be geometrically uniform on the sphere surface $S^2$.  
    
    In the AJWD mode, the shader computes the local Jacobian basis vectors to determine the primary axes of geometric stretch. The amplitude of the added noise $\mathbf{\eta}$ is then spatially modulated by the inverse of this stretch. Specifically, the algorithm identifies the principal axis of maximum geometric stretch. The noise amplitude along the minor axis is then boosted to match this maximum, ensuring that after the projection $\Phi^{-1}$ is applied, the noise footprint on the sphere remains circular. We contrast this against the standard baseline, where the noise amplitude is uniform across the texture domain $\Omega$.

    Crucially, to ensure the Jacobian weighting was theoretically ideal and free from screen-space aliasing, we did not rely on hardware derivatives (\texttt{ddx}/\texttt{ddy}). Instead, we implemented the exact \textit{analytic partial derivatives} of the Hemi-Octahedral mapping in the shader. The surface gradients $\frac{\partial \Phi}{\partial u}$ and $\frac{\partial \Phi}{\partial v}$ were computed explicitly from the $L_1$ norm definition, ensuring precise measurement of the local stretch even at low resolutions. We contrast this against the standard baseline, where the noise amplitude is uniform across the texture domain $\Omega$.
    
    To validate the masking efficiency, we employed standard Blue Noise. Crucially, we enforced a Triangular Probability Density Function (TPDF) by subtracting decorrelated samples (obtained via spatial offsetting) to generate the 2D offset vector. This ensures a constant noise floor and eliminates amplitude modulation, isolating the geometric projection $\Phi$ as the sole variable governing artifact visibility. While our experiments utilized Blue Noise, the geometric invariance identified in Eq. (3) applies universally to any noise distribution (e.g., White Noise, IGN).
    
    \subsubsection{Data Acquisition and Metric Evaluation}
    To ensure consistent objective evaluation using the FSIM metric, we implemented an automated capture system. Because the specular highlight moves relative to the view vector, a static crop would introduce framing bias. We developed a parallax-corrected cropping algorithm that iteratively solves for the surface point $\mathbf{p}$ where the reflection vector aligns with the camera. We found that a simple 3-pass convergence loop was sufficient to align the crop window with the specular highlight to sub-pixel accuracy, ensuring that the FSIM metric evaluated quantization noise rather than misalignment discrepancies. 
    
    For every combination of encoding (Cartesian vs. Hemi-Oct), dithering strategy (Uniform vs. AJWD), and bit-depth, the system renders and exports a $256 \times 256$ floating-point image of the highlight. These samples are then compared against a ``Ground Truth'' render (unquantized 32-bit float normals) using an offline Python script to compute the FSIM indices presented in Section 4.2.

\subsection{Results \& Evaluation}

    Visual Inspection (The Artifact Grid)
    
    Compare the max distortion places vs. min distortion places.

    Demonstrate that AJWD exposes simply adds way too much noise, or, if we reduce the amplitude manually, doesn't add enough in one axis.

    FSIM Error Maps: Heatmaps showing perceptual failure points. Expectation: Uniform Dithering has a lower mean FSIM score than AJWD.

    ALU cost comparison. Show that calculating det(J) or partial derivatives adds significant shader overhead ( 10-15 instructions) for negative visual gain.

\section{Discussion}

    Our results demonstrate a counter-intuitive but mathematically consistent reality: accounting for geometric distortion when dithering non-conformal projections is unnecessary. This provides a formal justification for the industry's reliance on simple uniform noise. The "luck" of standard implementations is actually a property of linear approximation: the distortion acts as an automatic spatial modulator.
    
    \subsection{The Intuition Trap: Integration vs. Masking}
    
    The hypothesis that dithering requires Jacobian weighting stems from a conflation of two distinct signal processing goals: \textit{energy preservation} (integration) and \textit{artifact masking} (quantization).
    
    In contexts such as Monte Carlo integration or texture filtering, the Jacobian determinant is critical. When mapping a signal from a parametric domain $\Omega$ to a surface $S^2$, one must weigh samples by $1/J(\mathbf{u})$ to ensure that the integral of the signal remains constant despite the distortion of the domain. This is the standard intuition for graphics engineers: "density is changing, so we must compensate."
    
    However, dithering is not an integration problem; it is a local contrast problem. The goal is not to preserve the total energy of the noise over the sphere, but to maintain the local ratio between the noise amplitude and the quantization step size (the SNR). As demonstrated in our theoretical analysis, the projection operator $\Phi$ applies the same local linear transformation $\mathbf{J}_{\Phi}$ to both the signal error and the added noise. 
    
    Consequently, applying an inverse-Jacobian weight to the noise creates a \textit{category error}. It attempts to equalize the absolute noise magnitude on the sphere, ignoring the fact that the quantization artifacts themselves have been stretched. By forcing the noise to be isotropic and uniform in regions where the error is anisotropic and magnified, the "corrected" approach breaks the covariance required for effective masking. We term this the "Intuition Trap": the assumption that a uniform output requires a non-uniform input, when in reality, a covariant input is required to mask a covariant error.

    \subsection{Temporal Stability and Supersampling}

    It is also necessary to contextualize these findings within the scope of Temporal Anti-Aliasing (TAA), which is now ubiquitous in real-time rendering. As noted by Cigolle et al., octahedral encodings can exhibit "wobbly" temporal artifacts because the quantization grid is fixed to the object's local frame, causing the discrete normal values to snap as the camera rotates.
    
    While dithering converts this structured snapping into high-frequency noise, it relies on TAA to accumulate these noisy samples into a stable image. One might hypothesize that Jacobian weighting could aid this process by normalizing the noise variance fed into the TAA history rectification clamp. However, our observations suggest the opposite. 
    
    By modulating noise amplitude based on surface location (mid-quadrant vs. diagonal), AJWD introduces a spatial variance in the noise floor. As an object rotates, a specific surface point moves through regions of varying Jacobian density. If the dithering strategy modulates amplitude based on this density, the noise level on that specific surface point pulsates over time. This variance complicates the heuristics used by TAA history rejection (neighborhood clamping), potentially leading to ghosting or instability. Uniform dithering provides a consistent noise baseline that is easier for temporal accumulation to model and reject.
    
\section{Conclusion}

    We have presented a mathematical proof and experimental validation for the invariance of dithering efficiency under Hemi-Octahedral projection. By contrasting standard methods against a Jacobian-corrected theoretical model, we demonstrated that the projection operator preserves the local Signal-to-Noise Ratio by deforming both signal and noise identically.
    
    This finding resolves the apparent paradox of why uniform dithering works well on distorted maps: the geometric distortion is not a hindrance to masking, but a covariant feature. We conclude that the standard approach—16-bit Hemi-Oct with Uniform noise—is not just an engineering compromise, but the Pareto-optimal solution for G-Buffers.

\section{References}

\end{document}
