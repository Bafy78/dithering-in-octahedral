\documentclass{article}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{bm}
\usepackage{array}
\usepackage[export]{adjustbox}

\title{The Invariance of Dithering Efficiency Under Non-Conformal Projections: A Case Study on Octahedral Normal Maps}
\author{Rémi Millerat-{}-Gallot}
\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
    Octahedral encoding, standard for normal map compression, significantly distorts surface density, yet uniform dithering remains the industry default. This paper provides a mathematical proof for the invariance of dithering efficiency under such non-conformal projections. By contrasting standard methods against a Jacobian-weighted theoretical foil, we demonstrate that explicit density correction is mathematically redundant. Our analysis reveals that the projection transforms both dithering noise and quantization error identically, preserving the local Signal-to-Noise Ratio. We conclude that standard uniform dithering is the theoretically optimal strategy for masking quantization artifacts.
\end{abstract}

\section{Introduction}

    Real-time rendering heavily relies on optimizing memory bandwidth, making vector quantization essential. Octahedral encodings have replaced older methods like spherical coordinates to become the industry standard for G-Buffers, widely adopted by engines like Unity (HDRP) and widely used for mesh storage (UE5 Nanite) and mobile G-Buffers. These methods dominate because they efficiently map 3D unit vectors to a compact 2D domain without the generic ``pole'' artifacts of polar systems. By packing normals into cost-effective formats like RG16 or RG8, they offer the optimal trade-off between storage size and computational cost.

    However, the efficiency of Hemi-Octahedral encoding comes at the cost of geometric uniformity. The mapping from the 2D square domain $\Omega$ to the unit sphere $S^2$ is neither conformal nor area-preserving. Consequently, the pixel density---the surface area on the sphere represented by a single texel---varies significantly across the projection. In regions such as the mid-quadrants, the mapping stretches the domain, causing a fixed quantization step in texture space to manifest as a magnified geometric error on the sphere surface. This results in a spatially inconsistent error distribution, where quantization artifacts (banding) are drastically more visible in specific regions of the view frustum than others.
    
    This non-uniformity invites a compelling question regarding signal processing: why does uniform noise in texture space yield high-quality masking on the sphere, despite the severe geometric distortion? Intuitively, one might assume that a uniform noise floor in texture space would result in non-uniform, ineffective masking on the sphere surface. This intuition would suggest a \textit{Jacobian-weighted} approach: modulating the noise amplitude proportionally to the local distortion to normalize the noise energy.

    This paper provides the formal mathematical proof for why such complexity is unnecessary. We utilize Anisotropic Jacobian-Weighted Dithering (AJWD) as a theoretical baseline to investigate the behavior of noise under non-conformal mappings. Our analysis demonstrates that Uniform PDF Dithering remains the optimal strategy because the projection operator transforms both the dithering noise and the quantization error identically. Consequently, the local Signal-to-Noise Ratio (SNR) remains invariant. We define the ``Anisotropy Fallacy''---the mistaken belief that isotropic noise is desirable on the sphere surface---and show that standard uniform dithering naturally satisfies the requirement for covariant masking, rendering explicit corrections computationally wasteful.

\section{Related Work}

\subsection{Unit Vector Encodings}
    Efficiently mapping the continuous unit sphere $S^2$ to a discrete 2D domain $\Omega$ is a foundational problem in real-time rendering. While the concept of perturbing surface normals via texture mapping was introduced by Blinn~\cite{blinn1978}, modern pipelines utilize explicit unit vector storage rather than height-map derivatives. Historical approaches relied on spherical coordinates (azimuth and inclination), but this parameterization suffers from the ``singularity problem'': precision density tends toward infinity at the poles while becoming sparse at the equator. As confirmed in the comprehensive survey by Cigolle et al.~\cite{cigolle2014}, this non-uniformity creates view-dependent artifacts and wastes significant precision.
    
    To address these inherent geometric inefficiencies, Meyer et al.~\cite{meyer2010} introduced Octahedral Normal Vectors (ONV) in 2010. By projecting the sphere onto an inscribed octahedron using the $L_1$ norm and unwrapping it to a square, they eliminated the need for expensive transcendental functions while ensuring a nearly uniform distribution of precision. By 2026, Octahedral encoding has superseded older methods to become the industry gold standard.

\subsection{Quantization and Dithering}
    Quantization artifacts, manifesting as Mach bands, are an inherent consequence of truncating high-precision signals to compact formats like 8-bit or 10-bit G-Buffers. Dithering mitigates this by adding noise prior to quantization, effectively decorrelating the discretization error from the signal and converting low-frequency structured artifacts into high-frequency noise.

    The perceptual quality of dithering is governed by two primary properties: the Power Spectral Density (PSD) and the Probability Density Function (PDF). Spatially, modern real-time rendering favors distributions with high-frequency characteristics (such as Blue Noise~\cite{ulichney1993} or Interleaved Gradient Noise~\cite{jimenez2014}) to shift error into frequency bands where the Human Visual System is least sensitive. Regarding amplitude, simple uniform distributions are insufficient; a Triangular Probability Distribution (TPDF) is theoretically required to ensure a constant noise floor and eliminate signal-dependent noise modulation~\cite{lipshitz1992}.

\subsection{Perceptual Metrics in Rendering}
    The objective evaluation of image fidelity requires metrics that align with the Human Visual System (HVS), rather than simple pixel-difference calculations. While traditional metrics like Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) remain in wide use due to their simplicity, they operate solely on image intensity and correlate poorly with subjective fidelity ratings \cite{zhang2011}. To address this, structural metrics such as SSIM were introduced to capture the loss of image structure. However, standard SSIM treats all image regions with equal importance during pooling, failing to account for the spatially varying sensitivity of the HVS.

    For this study, we utilize the Feature Similarity Index (FSIM) proposed by Zhang et al.~\cite{zhang2011}. As noted in recent comprehensive surveys, despite the emergence of deep learning-based methods, HVS-based statistical metrics like FSIM remain highly relevant for their interpretability and robustness \cite{ma2025}. FSIM is grounded in physiological evidence that the HVS perceives images primarily through low-level features rather than absolute pixel values. It employs \textbf{Phase Congruency (PC)} as a primary feature to detect stable structures (such as edges and zero crossings) where Fourier waves have congruent phases, and \textbf{Gradient Magnitude (GM)} as a secondary feature to capture contrast information.

    Crucially, FSIM utilizes the PC map to weight the final quality score, assigning higher importance to areas with significant structural information. This makes it particularly effective for evaluating the ``banding'' artifacts inherent to normal map quantization. In the TID2008 benchmark, FSIM demonstrated high consistency with human judgment, achieving an SROCC of 0.8564 for ``quantization noise'', outperforming other advanced structural metrics like VIF and MS-SSIM \cite{zhang2011}. This validates the metric as a theoretically robust arbiter for structural artifacts where pure intensity metrics might fail to capture the perceptual severity of banding.

\section{Theoretical Analysis}
    \subsection{Geometric Properties of the Projection}
    
    We define the Hemi-Octahedral projection as a mapping $\Phi: \Omega \rightarrow S^2$, where the parametric domain is the square $\Omega = [-1, 1]^2$. 
    
    The construction projects the hemisphere onto the inscribed octahedron ($L_1$ sphere) and applies a 45-degree rotation to map the resulting diamond shape to the axis-aligned square. For a domain coordinate $\mathbf{u} = [u, v]^\top$, we first recover the un-rotated planar coordinates $x', y'$:
    
    \begin{equation}
        x' = \frac{u + v}{2}, \quad y' = \frac{u - v}{2}.
    \end{equation}
    The intermediate $L_1$ vector $\mathbf{p}$ is then reconstructed by projecting onto the $z=1-L_1$ plane:

    \begin{equation}
        \mathbf{p} = \left[ x', \ y', \ 1 - |x'| - |y'| \right]^\top
    \end{equation}
    
    Finally, the mapping is the normalization $\Phi(\mathbf{u}) = \mathbf{p} / \|\mathbf{p}\|_2$. 
    
    Note that unlike full Octahedral encodings, the Hemi-Octahedral variant assumes $z \ge 0$ and requires no folding of the negative hemisphere.
    
    To analyze the interaction between this mapping and quantization noise, we must quantify the local geometric distortion. This is fully characterized by the Jacobian matrix of the projection, $\mathbf{J}_{\Phi}(\mathbf{u}) = [\frac{\partial \Phi}{\partial u}, \frac{\partial \Phi}{\partial v}]$. Two specific properties of $\mathbf{J}_{\Phi}$ are critical to our analysis:
    
    \begin{enumerate}
        \item \textbf{Area Scaling (Density):} The determinant $J(\mathbf{u}) = \sqrt{\det(\mathbf{J}_{\Phi}^\top \mathbf{J}_{\Phi})}$ represents the local expansion factor. As shown in Figure \ref{fig:hemi_distortion}, the mapping is strictly non-uniform. The density fluctuates from $J \approx 0.866$ at the pole ($\mathbf{u}=0$) to $J \approx 2.6$ in the ``mid-quadrant'' regions, indicating that precision density varies by a factor of $3\times$ across the view frustum.
        \begin{figure}[ht]
            \centering
            \includegraphics[height=7cm]{hemi_oct_distortion_plot.png}
            \caption{Heatmap of the Jacobian Determinant $J(\mathbf{u})$ for Hemi-Octahedral projection.}
            \label{fig:hemi_distortion}
        \end{figure}

        \item \textbf{Anisotropy (Shape):} Crucially, the mapping is not conformal. Outside of the center, and specifically around the diagonals, the condition number of $\mathbf{J}_{\Phi}$ increases, implying that the basis vectors are orthogonal neither in direction nor in magnitude. This anisotropic distortion stretches the local geometry significantly along one principal axis.
        \begin{figure}[ht]
            \centering
            \includegraphics[height=7cm]{hemi_oct_anisotropy_plot.png}
            \caption{Visualization of Anisotropy in the Hemi-Octahedral projection.}
            \label{fig:hemi_anisotropy}
        \end{figure}
    \end{enumerate}
    
    Consequently, a square quantization cell (or a uniform noise kernel) in the texture domain $\Omega$ does not map to a square patch on the sphere $S^2$. Instead, it is transformed into an elongated, parallelogram-shaped streak aligned with the major eigenvector of $\mathbf{J}_{\Phi}$. This structural deformation is the primary factor governing artifact visibility.
    
\subsection{Signal-to-Noise Invariance and the Anisotropy Fallacy}
    
    Having characterized the distortion, we now address the central hypothesis of this study: whether the dithering noise amplitude should be spatially modulated (weighted by $J(\mathbf{u})^{-1}$) to compensate for the variable density of the projection.
    
    We model the quantization process on the texture grid as the addition of a discretization error vector $\mathbf{e}_{\Omega}$ and a dithering noise vector $\bm{\eta}_{\Omega}$. Since the quantization step $\Delta$ is infinitesimal relative to the curvature of the manifold, the projection $\Phi$ is well-approximated by its first-order Taylor expansion. The resulting geometric error vectors on the sphere, $\mathbf{e}_{S^2}$ and $\bm{\eta}_{S^2}$, are thus linear transformations of their domain counterparts:
    
    \begin{equation}
        \mathbf{e}_{S^2} \approx \mathbf{J}_{\Phi}(\mathbf{u}) \, \mathbf{e}_{\Omega} \quad \text{and} \quad \bm{\eta}_{S^2} \approx \mathbf{J}_{\Phi}(\mathbf{u}) \, \bm{\eta}_{\Omega}
    \end{equation}
    
    This linearity reveals two fundamental reasons why explicit density correction is theoretically unsound.

    \subsubsection{Magnitude Invariance}
    
    The efficacy of dithering is determined by the local Signal-to-Noise Ratio (SNR), defined here as the ratio between the magnitude of the projected noise and the magnitude of the projected quantization step. Substituting the linear approximations, we observe that the Jacobian operator scales both terms:
    
    \begin{equation}
        \text{SNR}(\mathbf{u}) = \frac{\| \bm{\eta}_{S^2} \|_2}{\| \mathbf{e}_{S^2} \|_2} \approx \frac{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \bm{\eta}_{\Omega} \|_2}{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{e}_{\Omega} \|_2}
    \end{equation}
    
    Since $\mathbf{e}_{\Omega}$ and $\bm{\eta}_{\Omega}$ are defined in the same parametric space, the operator $\mathbf{J}_{\Phi}$ deforms their respective probability density functions identically.

    To quantify the stability of this relationship, we define the \textit{Efficiency Ratio} $R(\mathbf{u})$ as the normalized quotient of the noise expansion factor to the error expansion factor along the principal geometric axes:
    \begin{equation}
        R(\mathbf{u}) = \frac{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \bm{\eta}_{\Omega} \|_2 / \| \bm{\eta}_{\Omega} \|_2}{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{e}_{\Omega} \|_2 / \| \mathbf{e}_{\Omega} \|_2}
    \end{equation}
    
    As validated in Figure \ref{fig:efficiency_ratio}, the efficiency ratio remains stochastically invariant across the domain. The expansion or compression of the surface ($J(\mathbf{u})$) automatically modulates the noise level in exact proportion to the error level. No external weighting is required to maintain the masking threshold.

    \begin{figure}[ht]
        \centering
        \includegraphics[width=\textwidth]{efficiency_ratio_plot.png}
        \caption{Scatter plot of the Efficiency Ratio $R(\mathbf{u})$ across the domain. The ratio is invariant at $\approx 1.0$, indicating that projected noise scales exactly with projected error.}
        \label{fig:efficiency_ratio}
    \end{figure}

    \subsubsection{The Anisotropy Fallacy}
    
    The failure of density-corrected dithering is most acute when considering the \textit{shape} of the artifacts. The \textit{Jacobian-Weighted} strategy operates on the intuition that noise on the sphere should be isotropic (circular). However, as established in Section 3.1, the quantization error $\mathbf{e}_{S^2}$ is highly anisotropic; it manifests as elongated ``streaks'' aligned with the major eigenvector of $\mathbf{J}_{\Phi}$.
    
    Optimal masking requires the noise footprint to geometrically match the error footprint. We contrast the two approaches:
    
    \begin{itemize}
        \item \textbf{Uniform Dithering (Optimal):} The uniform noise $\bm{\eta}_{\Omega}$ is naturally stretched by $\mathbf{J}_{\Phi}$ into the same anisotropic shape as the error. The noise ``streak'' perfectly overlays the quantization ``streak'', ensuring consistent coverage.
        \item \textbf{Isotropic Correction (Sub-optimal):} By attempting to force the projected noise $\bm{\eta}_{S^2}$ to be circular, this strategy introduces a geometric mismatch. Along the axis of stretching, the circular noise is insufficiently long to cover the elongated banding artifact (under-dithering), while along the axis of compression, the noise is excessively wide (over-dithering).
    \end{itemize}
    
    We conclude that uniform dithering in the parameter space is the Pareto-optimal strategy. The geometric distortion acts not as a hindrance, but as an automatic spatial modulator that preserves the necessary covariance between signal and noise.

\section{Experimental Methodology}

\subsection{Implementation}

    To empirically validate the theoretical invariance proposed in Section 3, we developed a custom high-precision rendering framework using the WebGPU API and Three.js. The testing environment is designed to isolate quantization artifacts from other sources of error (such as geometric aliasing or texture filtering) and to provide a ``stress test'' scenario where normal precision is the sole determinant of visual quality.
    
    \subsubsection{Rendering Pipeline and Stress Test Configuration}
    The test scene consists of a highly tessellated sphere ($256 \times 256$ segments) to ensure that vertex interpolation artifacts remain negligible compared to pixel-stage quantization errors. We employ a Blinn-Phong specular model~\cite{blinn1977} with a high exponent to simulate a low-roughness material (perceptual roughness $r \approx 0.15$, mapping to a Blinn-Phong exponent of $s \approx 4000$). This configuration serves as the worst-case scenario for normal mapping: narrow specular highlights act as high-frequency amplifiers for low-frequency quantization steps, rendering ``banding'' artifacts immediately visible to the human eye.
    
    The quantization pipeline is implemented in a custom WGSL fragment shader. Instead of relying on hardware texture formats (which offer fixed precision), we simulate arbitrary bit-depths programmatically. The high-precision view-space normal $\mathbf{n}_{view}$ is projected into the Hemi-Octahedral domain $\Omega$, resulting in continuous UV coordinates. We then apply dithering noise $\bm{\eta}$ and round the signal to simulating $k$-bit fixed-point storage:
    
    \begin{equation}
        \mathbf{u}_{quant} = \frac{\operatorname{round}(\frac{\Phi^{-1}(\mathbf{n}_{view})+1}{2} \cdot (2^k - 1)  + \bm{\eta})}{2^k - 1}
    \end{equation}
    
    The quantized coordinate $\mathbf{u}_{quant}$ is then decoded back to a unit vector $\mathbf{n}_{decoded}$ for lighting calculations. This \textit{in silico} quantization allows us to instantly toggle between 8-bit, 10-bit, and 16-bit precision. Furthermore, because the simulation operates on floating-point bit-depths, we are able to sweep precision continuously (e.g., assessing transition artifacts at 9.5 bits) rather than being constrained to the integer steps of hardware formats.
    
    \subsubsection{The Theoretical Foil: Anisotropic Jacobian-Weighted Dithering (AJWD)}
    To rigorously validate the robustness of standard dithering, we implemented a counter-factual algorithm: Anisotropic Jacobian-Weighted Dithering (AJWD). While not a standard industrial technique, AJWD serves as the rigorous embodiment of the ``isotropic intuition'': the hypothesis that noise should be geometrically uniform on the sphere surface $S^2$.  
    
    In the AJWD mode, the shader computes the local Jacobian basis vectors to determine the primary axes of geometric stretch. The amplitude of the added noise $\bm{\eta}$ is then spatially modulated by the inverse of this stretch. Specifically, the algorithm identifies the principal axis of maximum geometric stretch. The noise amplitude along the minor axis is then boosted to match this maximum, ensuring that after the projection $\Phi^{-1}$ is applied, the noise footprint on the sphere remains circular. We contrast this against the standard baseline, where the noise amplitude is uniform across the texture domain $\Omega$.

    Crucially, to ensure the Jacobian weighting was theoretically ideal and free from screen-space aliasing, we did not rely on hardware derivatives (\texttt{ddx}/\texttt{ddy}). Instead, we implemented the exact \textit{analytic partial derivatives} of the Hemi-Octahedral mapping in the shader. The surface gradients $\frac{\partial \Phi}{\partial u}$ and $\frac{\partial \Phi}{\partial v}$ were computed explicitly from the $L_1$ norm definition, ensuring precise measurement of the local stretch even at low resolutions.
    
    To validate the masking efficiency, we employed standard Blue Noise. Crucially, we enforced a Triangular Probability Density Function (TPDF) by subtracting decorrelated samples (obtained via spatial offsetting) to generate the 2D offset vector. This ensures a constant noise floor and eliminates amplitude modulation, isolating the geometric projection $\Phi$ as the sole variable governing artifact visibility. While our experiments utilized Blue Noise~\cite{peters2016}, the geometric invariance identified in Eq. (3) applies universally to any noise distribution (e.g., White Noise, IGN).

    We also evaluated scalar Jacobian-Weighted Dithering (scaling isotropic noise amplitude by $\mathbf{J}(\mathbf{u})$. This method yielded strictly inferior results, producing excessive noise in high-distortion regions without improving masking efficiency, and was thus excluded from the detailed comparison.
    
    \subsubsection{Data Acquisition and Metric Evaluation}
    To ensure consistent objective evaluation using the FSIM metric, we implemented an automated capture system. Because the specular highlight moves relative to the view vector, a static crop would introduce framing bias. We developed a parallax-corrected cropping algorithm that iteratively solves for the surface point $\mathbf{p}$ where the reflection vector aligns with the camera. We found that a simple 3-pass convergence loop was sufficient to align the crop window with the specular highlight to sub-pixel accuracy, ensuring that the FSIM metric evaluated quantization noise rather than misalignment discrepancies. 
    
    For every combination of encoding (Cartesian vs. Hemi-Oct), dithering strategy (Uniform vs. AJWD), and bit-depth, the system renders and exports a $256 \times 256$ floating-point image of the highlight. These samples are then compared against a ``Ground Truth'' render (unquantized 32-bit float normals) using an offline Python script to compute the FSIM indices presented in Section 4.2.

\subsection{Results and Evaluation}

    \subsubsection{Visual Inspection: The Artifact Grid}
    
    \begin{figure}[ht]
        \centering
        \setlength{\tabcolsep}{0pt} 
        % --- Local helper command for 50% center zoom ---
        \newcommand{\zoomimg}[1]{%
            \adjincludegraphics[
                width=0.185\textwidth, 
                trim={.3\width} {.3\height} {.3\width} {.3\height}, 
                clip,
            valign=m
            ]{#1}%
        }
        
        \begin{tabular}{c c c c c c}
            % --- COLUMN HEADERS ---
            & \footnotesize Ground Truth
            & \footnotesize No Dither 
            & \footnotesize Uniform 
            & \footnotesize AJWD 
            & \footnotesize AJWD ($0.6\times$) \\
    
            % --- ROW 1: POLE ---
            \rotatebox[origin=c]{90}{\footnotesize \textbf{Pole}} &
            \zoomimg{pole_gt} &
            \zoomimg{pole_no_dither} &
            \zoomimg{pole_uniform} &
            \zoomimg{pole_ajwd_full} \\
    
            % --- ROW 2: MID-QUADRANT ---
            \rotatebox[origin=c]{90}{\footnotesize \textbf{Mid-Quad}} &
            \zoomimg{mid_gt} &
            \zoomimg{mid_no_dither} &
            \zoomimg{mid_uniform} &
            \zoomimg{mid_ajwd_full} &
            \zoomimg{mid_ajwd_norm} \\
    
            % --- ROW 3: DIAGONAL ---
            \rotatebox[origin=c]{90}{\footnotesize \textbf{Diagonal}} &
            \zoomimg{diag_gt} &
            \zoomimg{diag_no_dither} &
            \zoomimg{diag_uniform} &
            \zoomimg{diag_ajwd_full} &
            \zoomimg{diag_ajwd_norm} \\
            
        \end{tabular}
        
        \caption{Visual artifact comparison at 8-bit precision. \textbf{Rows:} Regions of varying distortion. \textbf{Columns:} Different type of dithering strategies vs Ground Truth.}
        \label{fig:artifact_grid}
    \end{figure}
    
    To isolate the perceptual impact of the dithering strategies, we captured high-dynamic-range crops of the specular highlight in three critical regions: the Pole (minimal distortion), the Mid-Quadrants (maximum density loss), and the Diagonal (maximum anisotropy). Figure \ref{fig:artifact_grid} presents these regions across five distinct modes, including a Ground Truth baseline.
    
    \textbf{Energy Normalization and the 0.6x Factor}
    A direct comparison is complicated by the fact that AJWD boosts noise amplitude to correct for distortion. To prevent AJWD from simply overpowering the signal with excessive energy, we introduced a "Normalized" condition (Column 5). We empirically lowered the global amplitude to $0.6\times$ so that the noise level along the \textit{boosted} axis (the direction of correction) roughly matched the baseline intensity of the Uniform approach. This setup isolates the geometric trade-off: by clamping the maximum noise visibility to standard levels, we expose whether the un-boosted axis retains enough energy to mask the quantization steps.    
    
    \textbf{Analysis of Failure Modes}
    The results reveal a clear dichotomy between geometric intent and perceptual reality:
    
    \begin{enumerate}
        \item \textbf{The Efficiency of Uniformity (Column 3):} Across all regions, standard Uniform dithering proves robust. Even in the Mid-Quadrants, where the Jacobian determinant peaks ($J \approx 2.6$) and precision density is lowest, the projection operator naturally scales the noise in exact proportion to the widened quantization steps. In the high-anisotropy Diagonal region, the noise is stretched into elongated streaks that perfectly overlay the quantization banding, effectively masking artifacts without requiring parameter tuning.
    
        \item \textbf{The Cost of Isotropy (Column 4):} Full-amplitude AJWD successfully masks artifacts but at the cost of excessive texture grain. In the Diagonal region, the algorithm forces the noise footprint to be circular despite the underlying error being highly anisotropic. This results in higher visible noise than the uniform version for no visual gain.
    
        \item \textbf{The Geometric Mismatch (Column 5):} The failure of the isotropic hypothesis is most evident in the Normalized ($0.6\times$) AJWD case. When the energy budget is constrained to match the standard approach, the isotropic shape becomes a liability. In both the Mid-Quadrant and Diagonal regions, the circular noise kernel is geometrically insufficient to span the major axis of the projected quantization step ("under-dithering"). Consequently, clear banding artifacts re-emerge, proving that masking power depends on covariant alignment, not just noise amplitude.
    \end{enumerate}

    \subsubsection{Quantitative Analysis (FSIM)}

    To provide an objective metric for these perceptual observations, we computed the Feature Similarity Index (FSIM) for each sample against the 32-bit ground truth. FSIM was chosen specifically for its sensitivity to structural artifacts (Phase Congruency) over simple intensity differences.
    
    Table \ref{tab:fsim_scores} summarizes the results, including a comparison between Rectangular (RPDF) and Triangular (TPDF) probability density functions.
    
    \begin{table}[ht]
        \centering
        \begin{tabular}{lccc}
        \hline
        \textbf{Method} & \textbf{Pole} & \textbf{Mid-Quadrant} & \textbf{Diagonal} \\ \hline
        No Dither & 0,8859 & 0,8683 & 0,9149 \\
        Uniform (RPDF) & 0,9357 & 0,9173 & 0,9463 \\
        \textbf{Uniform (TPDF / Standard)} & \textbf{0,9176} & \textbf{0,8932} & \textbf{0,9314} \\
        AJWD (Full) & 0,9176 & 0,8788 & 0,9199 \\ \hline
        \end{tabular}
        \caption{FSIM scores (Higher is Better). RPDF achieves the highest raw fidelity due to lower variance. Standard TPDF follows. AJWD (Full) scores significantly lower in distorted regions due to excessive texture grain}
        \label{tab:fsim_scores}
    \end{table}

    It is important to interpret the FSIM scores in the context of signal variance. As observed in Table 1, TPDF dithering yields slightly lower FSIM scores than RPDF. This is an expected consequence of TPDF possessing twice the variance of RPDF. We accept this marginal quantitative penalty to eliminate the perceptual artifact of signal-dependent noise modulation.

    However, AJWD demonstrates a breakdown of this cost-benefit ratio. It incurs a significant FSIM penalty due to the Jacobian-weighted noise amplification, yet fails to achieve a higher structural similarity score than the Uniform baseline. This indicates that the additional energy injected by AJWD does not contribute to masking; it is purely 'wasted' variance. Unlike TPDF, which trades a small amount of score for a specific visual property (stationarity), AJWD sacrifices fidelity without bringing any improvement.
    
    ALU cost comparison. Show that calculating det(J) or partial derivatives adds significant shader overhead ( 10-15 instructions) for negative visual gain.

\section{Discussion}

    Our results demonstrate a counter-intuitive but mathematically consistent reality: accounting for geometric distortion when dithering non-conformal projections is unnecessary. This provides a formal justification for the industry's reliance on simple uniform noise. The ``luck'' of standard implementations is actually a property of linear approximation: the distortion acts as an automatic spatial modulator.
    
    \subsection{The Intuition Trap: Integration vs. Masking}
    
    The hypothesis that dithering requires Jacobian weighting stems from a conflation of two distinct signal processing goals: \textit{energy preservation} (integration) and \textit{artifact masking} (quantization).
    
    In contexts such as Monte Carlo integration or texture filtering, the Jacobian determinant is critical. When mapping a signal from a parametric domain $\Omega$ to a surface $S^2$, one must weigh samples by $1/J(\mathbf{u})$ to ensure that the integral of the signal remains constant despite the distortion of the domain. This is the standard intuition for graphics engineers: ``density is changing, so we must compensate.''
    
    However, dithering is not an integration problem; it is a local contrast problem. The goal is not to preserve the total energy of the noise over the sphere, but to maintain the local ratio between the noise amplitude and the quantization step size (the SNR). As demonstrated in our theoretical analysis, the projection operator $\Phi$ applies the same local linear transformation $\mathbf{J}_{\Phi}$ to both the signal error and the added noise. 
    
    Consequently, applying an inverse-Jacobian weight to the noise creates a \textit{category error}. It attempts to equalize the absolute noise magnitude on the sphere, ignoring the fact that the quantization artifacts themselves have been stretched. By forcing the noise to be isotropic and uniform in regions where the error is anisotropic and magnified, the ``corrected'' approach breaks the covariance required for effective masking. We term this the ``Intuition Trap'': the assumption that a uniform output requires a non-uniform input, when in reality, a covariant input is required to mask a covariant error.

    \subsection{Temporal Stability and Supersampling}

    It is also necessary to contextualize these findings within the scope of Temporal Anti-Aliasing (TAA), which is now ubiquitous in real-time rendering. As noted by Cigolle et al.~\cite{cigolle2014}, octahedral encodings can exhibit temporal artifacts because the quantization grid is fixed to the object's local frame, causing the discrete normal values to snap as the camera rotates.
    
    While dithering converts this structured snapping into high-frequency noise, relying on TAA to accumulate these noisy samples into a stable image, we posit that Jacobian weighting would be detrimental to this process. Although one might hypothesize that normalizing variance aids the history rectification clamp, AJWD inherently couples noise amplitude to the local Jacobian determinant $J(\mathbf{u})$.
    
    Consequently, as an object rotates and a specific surface point traverses regions of varying density (e.g., moving from a diagonal to a mid-quadrant), the applied noise amplitude would fluctuate dynamically. This introduces a time-variant noise statistic that theoretically complicates the neighborhood clamping heuristics fundamental to modern TAA \cite{karis2014}, potentially inducing ghosting or rejection failure. In contrast, uniform dithering provides a consistent variance baseline, offering a theoretically more stable input for temporal accumulation.    
    
\section{Conclusion}

    We have presented a mathematical proof and experimental validation for the invariance of dithering efficiency under Hemi-Octahedral projection. By contrasting standard methods against a Jacobian-corrected theoretical model, we demonstrated that the projection operator preserves the local Signal-to-Noise Ratio by deforming both signal and noise identically.
    
    This finding resolves the apparent paradox of why uniform dithering works well on distorted maps: the geometric distortion is not a hindrance to masking, but a covariant feature. We conclude that the standard approach---16-bit Hemi-Oct with Uniform noise---is not just an engineering compromise, but the Pareto-optimal solution for G-Buffers.

\begin{thebibliography}{99}
    \bibitem{blinn1978}
    Blinn, J. F. (1978).
    Simulation of wrinkled surfaces.
    \textit{Proceedings of the 5th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '78)}, 286--292.

    \bibitem{cigolle2014}
    Cigolle, Z. H., Donow, S., Evangelakos, D., Mara, M., McGuire, M., \& Meyer, Q. (2014).
    A survey of efficient representations for independent unit vectors.
    \textit{Journal of Computer Graphics Techniques (JCGT)}, 3(2), 1--30.
    
    \bibitem{meyer2010}
    Meyer, Q., Süßmuth, J., Sußner, G., Stamminger, M., \& Greiner, G. (2010).
    On floating-point normal vectors.
    \textit{Computer Graphics Forum}, 29(4), 1405--1409.

    \bibitem{ulichney1993}
    Ulichney, R. (1993).
    The void-and-cluster method for dither array generation.
    \textit{Proceedings of SPIE 1913, Human Vision, Visual Processing, and Digital Display IV}, 332--343.
    
    \bibitem{jimenez2014}
    Jimenez, J. (2014).
    Next generation post processing in Call of Duty: Advanced Warfare.
    \textit{Proceedings of the Game Developers Conference (GDC)}.

    \bibitem{lipshitz1992}
    Lipshitz, S. P., Wannamaker, R. A., \& Vanderkooy, J. (1992). Quantization and Dither: A Theoretical Survey.
    \textit{Journal of the Audio Engineering Society}, 40(5), 355-375.
    
    \bibitem{zhang2011}
    Zhang, L., Zhang, L., Mou, X., \& Zhang, D. (2011).
    FSIM: A feature similarity index for image quality assessment.
    \textit{IEEE Transactions on Image Processing}, 20(8), 2378--2386.
    
    \bibitem{ma2025}
    Ma, C., Shi, Z., Lu, Z., Xie, S., Chao, F., \& Sui, Y. (2025).
    A survey on image quality assessment: Insights, analysis, and future outlook.
    \textit{arXiv preprint arXiv:2502.08540}.

    \bibitem{blinn1977}
    Blinn, J. F. (1977).
    Models of light reflection for computer synthesized pictures.
    \textit{Proceedings of the 4th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '77)}, 192--198.

    \bibitem{peters2016}
    Peters, C. (2016).
    Free Blue Noise Textures.
    \textit{Moments in Graphics}. Available at: \url{http://momentsingraphics.de/BlueNoise.html}

    \bibitem{karis2014}
    Karis, B. (2014).
    High Quality Temporal Supersampling.
    \textit{SIGGRAPH 2014 Advances in Real-Time Rendering in Games Course}.
    Available at: \url{http://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf}
\end{thebibliography}

\end{document}
