\documentclass{article}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}

\title{The Invariance of Dithering Efficiency Under Non-Conformal Projections: A Case Study on Octahedral Normal Maps}
\author{Rémi Millerat--Gallot}
\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
    Hemi-Octahedral encoding is standard for normal map compression, yet it significantly distorts the density of the projected surface. We investigated whether compensating for this distortion using Jacobian-weighted dithering could improve the masking of quantization artifacts compared to standard methods. Our analysis reveals that this hypothesis is incorrect: because the projection transforms the dithering noise and the quantization error identically, the signal-to-noise ratio remains constant without intervention. We conclude that standard uniform dithering is theoretically and practically superior, as explicit density corrections incur computational costs for no visual gain.
\end{abstract}

\section{Introduction}

    Real-time rendering heavily relies on optimizing memory bandwidth, making vector quantization essential. Octahedral and Hemi-Octahedral encodings have replaced older methods like spherical coordinates to become the industry standard for G-Buffers, widely adopted by engines such as Unreal Engine 5 and Unity. These methods dominate because they efficiently map 3D unit vectors to a compact 2D domain without the generic ``pole'' artifacts of polar systems. By packing normals into cost-effective formats like RG16 or RG8, they offer the optimal trade-off between storage size and computational cost.

    However, the efficiency of Hemi-Octahedral encoding comes at the cost of geometric uniformity. The mapping from the 2D square domain $\Omega$ to the unit sphere $S^2$ is neither conformal nor area-preserving. Consequently, the pixel density—the surface area on the sphere represented by a single texel—varies significantly across the projection. In regions such as the mid-quadrants, the mapping stretches the domain, causing a fixed quantization step in texture space to manifest as a magnified geometric error on the sphere surface. This results in a spatially inconsistent error distribution, where quantization artifacts (banding) are drastically more visible in specific regions of the view frustum than others.
    
    This non-uniformity invites a compelling, albeit incorrect, intuition regarding signal processing. If the geometric distortion varies, it follows that a uniform noise floor in texture space will yield non-uniform noise on the sphere. A natural hypothesis is that to achieve perceptually uniform masking, the dithering strategy must compensate for the projection's distortion. This line of reasoning suggests a \textit{Jacobian-weighted} approach: modulating the noise amplitude proportionally to the local distortion, in an attempt to match the noise energy to the magnified geometric error.

    This paper refutes the utility of such density-corrected dithering strategies. We present a theoretical proof and experimental validation demonstrating that Uniform PDF Dithering in the texture domain remains the optimal strategy for masking quantization artifacts, regardless of the projection's non-linearity.
    
    Our analysis shows that because the projection operator transforms both the dithering noise and the quantization error identically, the signal-to-noise ratio (SNR) remains invariant under the mapping. We identify the ``Anisotropy Fallacy''—the mistaken belief that isotropic noise is desirable on the sphere surface. In reality, the quantization artifacts themselves are anisotropic (stretched) by the projection; thus, optimal masking requires the noise to undergo the exact same stretching. We conclude that standard uniform dithering naturally satisfies this requirement, rendering explicit Jacobian corrections computationally wasteful and visually detrimental.

\section{Related Work}

\subsection{Unit Vector Encodings}
    Efficiently mapping the continuous unit sphere $S^2$ to a discrete 2D domain $\Omega$ is a foundational problem in real-time rendering. Historical approaches relied on spherical coordinates (azimuth and inclination), but this parameterization suffers from the ``singularity problem'': precision density tends toward infinity at the poles while becoming sparse at the equator. As confirmed in the comprehensive survey by Cigolle et al., this non-uniformity creates view-dependent artifacts and wastes significant precision.
    
    To address these inherent geometric inefficiencies, Meyer et al. introduced Octahedral Normal Vectors (ONV) in 2010. By projecting the sphere onto an inscribed octahedron using the $L_1$ norm and unwrapping it to a square, they eliminated the need for expensive transcendental functions while ensuring a nearly uniform distribution of precision. By 2026, Octahedral encoding (and its Hemi-Octahedral variant) has superseded older methods to become the industry gold standard. However, while macroscopically efficient, the mapping is not strictly isometric; it introduces local variations in pixel density (Jacobian determinant) that form the basis of our quantization analysis.

\subsection{Quantization \& Dithering: Overview of standard approaches (Bayer, Blue Noise, IGN)}
    
    Quantization artifacts, commonly manifested as ``color banding'' or Mach bands, arise when high-precision signals are truncated to lower bit-depths, such as standard 8-bit textures or compact G-Buffers . This loss of precision is particularly detrimental in regions of smooth gradients—such as skyboxes, fog, or normal maps on varying surfaces—where the quantization step size becomes perceptually distinct . While gamma correction can inadvertently expose these artifacts in dark areas if mishandled, they remain a fundamental consequence of the discretization process .
    
    The standard solution to this problem is dithering: the intentional addition of noise to the signal prior to quantization . This process effectively decorrelates the quantization error from the signal, converting structured, low-frequency banding artifacts into high-frequency noise, which is visually less objectionable. The effectiveness of dithering is governed by two primary characteristics: the probability density function (PDF) of the noise amplitude and its spatial spectral distribution (PSD).
    
    Regarding the amplitude distribution, simple uniform white noise (RPDF) is often insufficient as it preserves the mean error but fails to eliminate noise modulation, leaving areas of low noise where the signal aligns with quantization steps . Gjøl et al. demonstrate that a triangular probability distribution (TPDF), achieved by summing two uniform random variables (for texture-based noise) or analytically remapping the signal (for procedural noise), provides superior results . By extending the noise range (typically $[-1, 1]$ LSB), TPDF dithering ensures a constant noise floor, eliminating the visual modulation that occurs with uniform distributions .
    
    Spatially, the distribution of noise samples determines the perceptual quality of the dithered image. Several standard approaches dominate real-time rendering:
    
    \begin{itemize}
        \item \textbf{Ordered Dithering (Bayer Matrix):} Historically ubiquitous, Bayer dithering utilizes a recursive matrix to maximize signal difference in local neighborhoods . While computationally inexpensive and rich in high frequencies, its discrete spectral peaks often result in conspicuous, structured cross-hatch patterns that are prone to aliasing . Wronski notes that despite its legacy usage, ordered dithering is generally visually inferior to modern stochastic methods .
    
        \item \textbf{Blue Noise:} Considered the theoretical ideal for dithering, Blue Noise is characterized by a lack of low-frequency energy (a ``clumping'' free spectrum) and a uniform distribution of high frequencies. This spectral profile pushes quantization error into the frequency bands where the human visual system is least sensitive. However, generating high-quality Blue Noise typically requires pre-computed textures, imposing a memory bandwidth cost that can be prohibitive for specific passes.
    
        \item \textbf{Interleaved Gradient Noise (IGN):} Proposed by Jimenez in \textit{Call of Duty: Advanced Warfare}, IGN represents a procedural middle ground designed for optimal ALU efficiency. It utilizes a specific algebraic formulation to generate a high-frequency, varying pattern without texture lookups. Wronski highlights IGN as a highly effective real-time solution that mimics the favorable high-frequency characteristics of Blue Noise while avoiding the structured artifacts of Bayer matrices, making it particularly suitable for efficient shader-based dithering .
    \end{itemize}

\subsection{Perceptual Metrics in Rendering}
    The objective evaluation of image fidelity requires metrics that align with the Human Visual System (HVS), rather than simple pixel-difference calculations. Traditional metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) operate directly on image intensity and correlate poorly with subjective fidelity ratings.

    To address this, metrics like the Structural Similarity (SSIM) index were introduced to capture the loss of structure in an image, based on the hypothesis that the HVS is highly adapted to extract structural information. However, SSIM and its multi-scale variant (MS-SSIM) treat all positions in an image with equal importance during pooling, which fails to account for the varying contributions of different local regions to perception.
    
    For the evaluation of quantization artifacts in this study, we utilize the Feature Similarity Index (FSIM) proposed by Zhang et al.. FSIM is grounded in the physiological evidence that the HVS understands images mainly according to low-level features. The metric employs two primary features to characterize local quality:
    
    \begin{itemize} \item \textbf{Phase Congruency (PC):} A dimensionless measure of the significance of a local structure. PC is chosen because visually discernable features coincide with points where Fourier waves at different frequencies have congruent phases. This allows the model to detect stable features like edges and zero crossings. \item \textbf{Gradient Magnitude (GM):} Since PC is contrast invariant, FSIM employs GM as a secondary feature to encode contrast information, which affects the HVS perception of image quality. \end{itemize}
    
    While originally validated on general image databases, FSIM has demonstrated high consistency with subjective evaluations for specific distortion types relevant to this research. In the TID2008 benchmark, FSIM achieved a Spearman correlation (SROCC) of 0.8564 for "quantization noise" and 0.8566 for "additive Gaussian noise". This suggests the metric is theoretically robust for arbitrating between the structural distortion of quantization banding and the noise floor introduced by dithering strategies. Unlike SSIM, which weights all pixels equally, FSIM uses the PC map to weight the final quality score, positing that areas with high phase congruency (significant structure) are more important to the HVS when evaluating similarity.

\section{Theoretical Analysis}
    \subsection{Geometric Properties of the Projection}
    
    We define the Hemi-Octahedral projection as a mapping $\Phi: \Omega \rightarrow S^2$, where the parametric domain is the square $\Omega = [-1, 1]^2$. The construction flattens the sphere onto an inscribed octahedron (the $L_1$ sphere), and applies a 45-degree rotation to map the resulting diamond shape to the axis-aligned square, before normalizing the result. For a domain coordinate $\mathbf{u} = [u, v]^T$, the intermediate $L_1$ vector is given by $\mathbf{p} = [u, v, 1 - |u| - |v|]^T$ (with appropriate folding for the negative hemisphere), and the final mapping is the normalization $\Phi(\mathbf{u}) = \mathbf{p} / \|\mathbf{p}\|_2$.
    
    To analyze the interaction between this mapping and quantization noise, we must quantify the local geometric distortion. This is fully characterized by the Jacobian matrix of the projection, $\mathbf{J}_{\Phi}(\mathbf{u}) = [\frac{\partial \Phi}{\partial u}, \frac{\partial \Phi}{\partial v}]$. Two specific properties of $\mathbf{J}_{\Phi}$ are critical to our analysis:
    
    \begin{enumerate}
        \item \textbf{Area Scaling (Density):} The determinant $J(\mathbf{u}) = \sqrt{\det(\mathbf{J}_{\Phi}^T \mathbf{J}_{\Phi})}$ represents the local expansion factor. As shown in Figure \ref{fig:hemi_distortion}, the mapping is strictly non-uniform. The density fluctuates from $J \approx 0.6$ at the pole ($\mathbf{u}=0$) to $J \approx 2.55$ in the "mid-quadrant" regions, indicating that precision density varies by a factor of $4\times$ across the view frustum.
        \begin{figure}[h]
            \centering
            \includegraphics[height=7cm]{hemi_oct_distortion_plot.png}
            \caption{Heatmap of the Jacobian Determinant $J(\mathbf{u})$ for Hemi-Octahedral projection.}
            \label{fig:hemi_distortion}
        \end{figure}

        \item \textbf{Anisotropy (Shape):} Crucially, the mapping is not conformal. Outside of the center, and specifically around the diagonals, the condition number of $\mathbf{J}_{\Phi}$ increases, implying that the basis vectors are orthogonal neither in direction nor in magnitude. This anisotropic distortion stretches the local geometry significantly along one principal axis.
        \begin{figure}[h]
            \centering
            \includegraphics[height=7cm]{hemi_oct_anisotropy_plot.png}
            \caption{Visualization of Anisotropy in the Hemi-Octahedral projection.}
            \label{fig:hemi_anisotropy}
        \end{figure}
    \end{enumerate}
    
    Consequently, a square quantization cell (or a uniform noise kernel) in the texture domain $\Omega$ does not map to a square patch on the sphere $S^2$. Instead, it is transformed into an elongated, parallelogram-shaped streak aligned with the major eigenvector of $\mathbf{J}_{\Phi}$. This structural deformation is the primary factor governing artifact visibility.
    
\subsection{Signal-to-Noise Invariance and the Anisotropy Fallacy}
    
    Having characterized the distortion, we now address the central hypothesis of this study: whether the dithering noise amplitude should be spatially modulated (weighted by $J(\mathbf{u})^{-1}$) to compensate for the variable density of the projection.
    
    We model the quantization process on the texture grid as the addition of a discretization error vector $\mathbf{e}_{\Omega}$ and a dithering noise vector $\mathbf{\eta}_{\Omega}$. Since the quantization step $\Delta$ is infinitesimal relative to the curvature of the manifold, the projection $\Phi$ is well-approximated by its first-order Taylor expansion. The resulting geometric error vectors on the sphere, $\mathbf{e}_{S^2}$ and $\mathbf{\eta}_{S^2}$, are thus linear transformations of their domain counterparts:
    
    \begin{equation}
        \mathbf{e}_{S^2} \approx \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{e}_{\Omega} \quad \text{and} \quad \mathbf{\eta}_{S^2} \approx \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{\eta}_{\Omega}
    \end{equation}
    
    This linearity reveals two fundamental reasons why explicit density correction is theoretically unsound.

    \subsubsection{Magnitude Invariance}
    
    The efficacy of dithering is determined by the local Signal-to-Noise Ratio (SNR), defined here as the ratio between the magnitude of the projected noise and the magnitude of the projected quantization step. Substituting the linear approximations, we observe that the Jacobian operator scales both terms:
    
    \begin{equation}
        \text{SNR}(\mathbf{u}) = \frac{\| \mathbf{\eta}_{S^2} \|_2}{\| \mathbf{e}_{S^2} \|_2} \approx \frac{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{\eta}_{\Omega} \|_2}{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{e}_{\Omega} \|_2}
    \end{equation}
    
    Since $\mathbf{e}_{\Omega}$ and $\eta_{\Omega}$ are defined in the same parametric space, the operator $\mathbf{J}_{\Phi}$ deforms their respective probability density functions identically.

    To quantify the stability of this relationship, we define the \textit{Efficiency Ratio} $R(\mathbf{u})$ as the normalized quotient of the noise expansion factor to the error expansion factor along the principal geometric axes:
    \begin{equation}
        R(\mathbf{u}) = \frac{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{\eta}_{\Omega} \|_2 / \| \mathbf{\eta}_{\Omega} \|_2}{\| \mathbf{J}_{\Phi}(\mathbf{u}) \cdot \mathbf{e}_{\Omega} \|_2 / \| \mathbf{e}_{\Omega} \|_2}
    \end{equation}
    
    As validated in Figure \ref{fig:efficiency_ratio}, the efficiency ratio remains stochastically invariant across the domain. The expansion or compression of the surface ($J(\mathbf{u})$) automatically modulates the noise level in exact proportion to the error level. No external weighting is required to maintain the masking threshold.

    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{efficiency_ratio_plot.png}
        \caption{Scatter plot of the Efficiency Ratio $R(\mathbf{u})$ across the domain. The ratio is invariant at $\approx 1.0$, indicating that projected noise scales exactly with projected error.}
        \label{fig:efficiency_ratio}
    \end{figure}

    \subsubsection{The Anisotropy Fallacy}
    
    The failure of density-corrected dithering is most acute when considering the \textit{shape} of the artifacts. The \textit{Jacobian-Weighted} strategy operates on the intuition that noise on the sphere should be isotropic (circular). However, as established in Section 3.1, the quantization error $\mathbf{e}_{S^2}$ is highly anisotropic; it manifests as elongated "streaks" aligned with the major eigenvector of $\mathbf{J}_{\Phi}$.
    
    Optimal masking requires the noise footprint to geometrically match the error footprint. We contrast the two approaches:
    
    \begin{itemize}
        \item \textbf{Uniform Dithering (Optimal):} The uniform noise $\mathbf{\eta}_{\Omega}$ is naturally stretched by $\mathbf{J}_{\Phi}$ into the same anisotropic shape as the error. The noise "streak" perfectly overlays the quantization "streak," ensuring consistent coverage.
        \item \textbf{Isotropic Correction (Sub-optimal):} By attempting to force the projected noise $\mathbf{\eta}_{S^2}$ to be circular, this strategy introduces a geometric mismatch. Along the axis of stretching, the circular noise is insufficiently long to cover the elongated banding artifact (under-dithering), while along the axis of compression, the noise is excessively wide (over-dithering).
    \end{itemize}
    
    We conclude that uniform dithering in the parameter space is the Pareto-optimal strategy. The geometric distortion acts not as a hindrance, but as an automatic spatial modulator that preserves the necessary covariance between signal and noise.

\section{Experimental Methodology}

\subsection{Implementation}

    To empirically validate the theoretical invariance proposed in Section 3, we developed a custom high-precision rendering framework using the WebGPU API and Three.js. The testing environment is designed to isolate quantization artifacts from other sources of error (such as geometric aliasing or texture filtering) and to provide a ``stress test'' scenario where normal precision is the sole determinant of visual quality.
    
    \subsubsection{Rendering Pipeline \& Stress Test Configuration}
    The test scene consists of a highly tessellated sphere ($256 \times 256$ segments) to ensure that vertex interpolation artifacts remain negligible compared to pixel-stage quantization errors. We employ a Blinn-Phong specular model with a high exponent to simulate a low-roughness material (linear roughness $\alpha \approx 0.15$, mapping to a Blinn-Phong exponent of $s \approx 4000$). This configuration serves as the worst-case scenario for normal mapping: narrow specular highlights act as high-frequency amplifiers for low-frequency quantization steps, rendering ``banding'' artifacts immediately visible to the human eye.
    
    The quantization pipeline is implemented in a custom WGSL fragment shader. Instead of relying on hardware texture formats (which offer fixed precision), we simulate arbitrary bit-depths programmatically. The high-precision view-space normal $\mathbf{n}_{view}$ is projected into the Hemi-Octahedral domain $\Omega$, resulting in continuous UV coordinates. We then apply dithering noise $\mathbf{\eta}$ and round the signal to simulating $k$-bit fixed-point storage:
    
    \begin{equation}
        \mathbf{u}_{quant} = \frac{round(\frac{\Phi(\mathbf{n}_{view})+1}{2} \cdot (2^k - 1)  + \mathbf{\eta})}{2^k - 1}
    \end{equation}
    
    The quantized coordinate $\mathbf{u}_{quant}$ is then decoded back to a unit vector $\mathbf{n}_{decoded}$ for lighting calculations. This ``in-silico'' quantization allows us to instantly toggle between 8-bit, 10-bit, and 16-bit precision. Furthermore, because the simulation operates on floating-point bit-depths, we are able to sweep precision continuously (e.g., assessing transition artifacts at 9.5 bits) rather than being constrained to the integer steps of hardware formats.
    
    \subsubsection{The ``Strawman'' Algorithm: Anisotropic Jacobian-Weighted Dithering (AJWD)}
    To test the ``Anisotropy Fallacy,'' we implemented a novel, ostensibly ``corrected'' dithering algorithm: Anisotropic Jacobian-Weighted Dithering (AJWD). This algorithm represents the hypothesis that noise should be isotropic on the sphere surface $S^2$, and its energy should match the geometric error magnitude.
    
    In the AJWD mode, the shader computes the local Jacobian basis vectors to determine the primary axes of geometric stretch. The amplitude of the added noise $\mathbf{\eta}$ is then spatially modulated by the inverse of this stretch. Specifically, the algorithm identifies the principal axis of maximum geometric stretch. The noise amplitude along the minor axis is then boosted to match this maximum, ensuring that after the projection $\Phi^{-1}$ is applied, the noise footprint on the sphere remains circular. We contrast this against the standard baseline, where the noise amplitude is uniform across the texture domain $\Omega$.

    Crucially, to ensure the Jacobian weighting was theoretically ideal and free from screen-space aliasing, we did not rely on hardware derivatives (\texttt{ddx}/\texttt{ddy}). Instead, we implemented the exact \textit{analytic partial derivatives} of the Hemi-Octahedral mapping in the shader. The surface gradients $\frac{\partial \Phi}{\partial u}$ and $\frac{\partial \Phi}{\partial v}$ were computed explicitly from the $L_1$ norm definition, ensuring precise measurement of the local stretch even at low resolutions. We contrast this against the standard baseline, where the noise amplitude is uniform across the texture domain $\Omega$.
    
    We also integrated two distinct noise generation strategies to verify if spectral characteristics influence the geometric masking. For both methods, we ensured a Triangular Probability Density Function (TPDF) to eliminate noise modulation:
    \begin{itemize}
        \item \textbf{Texture-based Blue Noise:} Utilizing a $64 \times 64$ tiling texture to maximize high-frequency energy. We achieved TPDF by subtracting decorrelated samples (obtained via spatial offsetting) to generate the 2D offset vector.
        \item \textbf{Interleaved Gradient Noise (IGN):} A procedurally generated high-frequency pattern, preferred for its ALU efficiency in modern renderers. We utilized a lightweight analytic remapping function $f(x) = \frac{x}{\sqrt{|x|}} - \text{sgn}(x)$ to reshape the uniform distribution into a triangular one without the computational cost of a second PRNG sample.
    \end{itemize}
    
    \subsubsection{Data Acquisition and Metric Evaluation}
    To ensure consistent objective evaluation using the FSIM metric, we implemented an automated capture system. Because the specular highlight moves relative to the view vector, a static crop would introduce framing bias. We developed a parallax-corrected cropping algorithm that iteratively solves for the surface point $\mathbf{p}$ where the reflection vector aligns with the camera. We found that a simple 3-pass convergence loop was sufficient to align the crop window with the specular highlight to sub-pixel accuracy, ensuring that the FSIM metric evaluated quantization noise rather than misalignment discrepancies. 
    
    For every combination of encoding (Cartesian vs. Hemi-Oct), dithering strategy (Uniform vs. AJWD), and bit-depth, the system renders and exports a $256 \times 256$ floating-point image of the highlight. These samples are then compared against a ``Ground Truth'' render (unquantized 32-bit float normals) using an offline Python script to compute the FSIM indices presented in Section 4.2.

\subsection{Results \& Evaluation}

    Visual Inspection (The Artifact Grid)
    
    Compare the max distortion places vs. min distortion places.

    Demonstrate that AJWD exposes simply adds way too much noise, or, if we reduce the amplitude manually, doesn't add enough in one axis.

    FSIM Error Maps: Heatmaps showing perceptual failure points. Expectation: Uniform Dithering has a lower mean FSIM score than AJWD.

    ALU cost comparison. Show that calculating det(J) or partial derivatives adds significant shader overhead ( 10-15 instructions) for negative visual gain.

\section{Discussion}

    Our results demonstrate a counter-intuitive but mathematically consistent reality: accounting for geometric distortion when dithering non-conformal projections is not only unnecessary but detrimental. This finding invites a broader discussion on why the "Jacobian intuition" fails in this specific context and how these findings integrate with modern temporal supersampling pipelines.

    \subsection{The Intuition Trap: Integration vs. Masking}
    
    The hypothesis that dithering requires Jacobian weighting stems from a conflation of two distinct signal processing goals: \textit{energy preservation} (integration) and \textit{artifact masking} (quantization).
    
    In contexts such as Monte Carlo integration or texture filtering, the Jacobian determinant is critical. When mapping a signal from a parametric domain $\Omega$ to a surface $S^2$, one must weigh samples by $1/J(\mathbf{u})$ to ensure that the integral of the signal remains constant despite the distortion of the domain. This is the standard intuition for graphics engineers: "density is changing, so we must compensate."
    
    However, dithering is not an integration problem; it is a local contrast problem. The goal is not to preserve the total energy of the noise over the sphere, but to maintain the local ratio between the noise amplitude and the quantization step size (the SNR). As demonstrated in our theoretical analysis, the projection operator $\Phi$ applies the same local linear transformation $\mathbf{J}_{\Phi}$ to both the signal error and the added noise. 
    
    Consequently, applying an inverse-Jacobian weight to the noise creates a \textit{category error}. It attempts to equalize the absolute noise magnitude on the sphere, ignoring the fact that the quantization artifacts themselves have been stretched. By forcing the noise to be isotropic and uniform in regions where the error is anisotropic and magnified, the "corrected" approach breaks the covariance required for effective masking. We term this the "Intuition Trap": the assumption that a uniform output requires a non-uniform input, when in reality, a covariant input is required to mask a covariant error.

    \subsection{Temporal Stability and Supersampling}

    It is also necessary to contextualize these findings within the scope of Temporal Anti-Aliasing (TAA), which is now ubiquitous in real-time rendering. As noted by Cigolle et al., octahedral encodings can exhibit "wobbly" temporal artifacts because the quantization grid is fixed to the object's local frame, causing the discrete normal values to snap as the camera rotates.
    
    While dithering converts this structured snapping into high-frequency noise, it relies on TAA to accumulate these noisy samples into a stable image. One might hypothesize that Jacobian weighting could aid this process by normalizing the noise variance fed into the TAA history rectification clamp. However, our observations suggest the opposite. 
    
    By modulating noise amplitude based on surface location (mid-quadrant vs. diagonal), AJWD introduces a spatial variance in the noise floor. As an object rotates, a specific surface point moves through regions of varying Jacobian density. If the dithering strategy modulates amplitude based on this density, the noise level on that specific surface point pulsates over time. This variance complicates the heuristics used by TAA history rejection (neighborhood clamping), potentially leading to ghosting or instability. Uniform dithering provides a consistent noise baseline that is easier for temporal accumulation to model and reject.
    
\section{Conclusion}

    Summarize the findings: Complexity does not equal Quality.

    Final recommendation: Standard 16-bit Hemi-Oct with Uniform noise is the Pareto-optimal solution for G-Buffers.

\section{References}

\end{document}
